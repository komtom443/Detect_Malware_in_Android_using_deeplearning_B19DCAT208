import dgl.nn.pytorch as graph_nn
import torch.nn.functional as F
from torch import nn
from dgl.nn import Sequential

def get_layer(
        conv_algorithm: str,
        input_dimension: int,
        output_dimension: int
) -> nn.Module:
    support_algorithm = {
        "SAGEConv": graph_nn.SAGEConv(
            input_dimension,
            output_dimension,
            activation=F.relu,
            aggregator_type='mean',
            norm=F.normalize
        ),
        "GraphConv": graph_nn.GraphConv(
            input_dimension,
            output_dimension,
            activation=F.relu
        ),
        "GATConv": graph_nn.GATConv(
            input_dimension,
            output_dimension,
            activation=F.relu,
            num_heads=1
        )
    }
    conv_algorithm = conv_algorithm if conv_algorithm in support_algorithm.keys() else "GraphConv"
    return support_algorithm[conv_algorithm]

def get_embbeding_algorithm(
        input_dimension: int,
        conv_count=3,
        conv_algorithm='GraphConv'
):
    conv_dimensions = [353, 353, 353]
    conv_layers = []
    for layer in conv_dimensions[:conv_count]:
        conv_layers.append(get_layer(
            conv_algorithm,
            input_dimension,
            output_dimension=layer))
        input_dimension = layer
    conv_layers = Sequential(*conv_layers)
    return conv_layers