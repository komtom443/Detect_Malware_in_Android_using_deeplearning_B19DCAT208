import dgl
import sys
import torch
import argparse
import traceback
import joblib as J
import networkx as nx
import multiprocessing

from pathlib import Path
from androguard.misc import AnalyzeAPK
from random_forest_extractors import RandomForestExtractors


def process(source_file: Path, dest_dir: Path, index: int, total: int):
    try:
        mappings = {}
        file_name = source_file.stem
        _, _, dx = AnalyzeAPK(source_file)
        cg = dx.get_call_graph()
        rfe = RandomForestExtractors(nodes=cg.nodes)
        for node in cg.nodes():
            mappings[node] = rfe.get_features(node)

        nx.set_node_attributes(cg, mappings)
        cg = nx.convert_node_labels_to_integers(cg)
        dg = dgl.from_networkx(cg, node_attrs=['function_name'])
        dest_dir = dest_dir / f'{file_name}.fcg'
        dgl.data.utils.save_graphs(str(dest_dir), [dg])
        print(f"Processed ({index}/{total}) {source_file}")
    except:
        print(f"Error while processing {source_file}")
        traceback.print_exception(*sys.exc_info())
        return


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Preprocess APK Dataset into Graphs')

    parser.add_argument(
        '-m', '--mode',
        help='This will auto take "train", "test", "custom" value',
        default='custom'
    )

    parser.add_argument(
        '-s', '--source-dir',
        help='The directory containing apks=',
    )

    parser.add_argument(
        '-d', '--dest-dir',
        help='The directory to store processed graphs',
    )

    args = parser.parse_args()

    if args.mode == 'custom':
        source_dir = Path(args.source_dir)
        dest_dir = Path(args.dest_dir)
    elif args.mode == 'test':
        source_dir = Path("dataset/raw/test")
        dest_dir = Path("dataset/rdg/test")
    elif args.mode == 'train':
        source_dir = Path("dataset/raw/train")
        dest_dir = Path("dataset/rdg/train")
    else:
        raise ValueError(f'--mode="{args.mode}" not found')

    if not source_dir.exists():
        raise FileNotFoundError(f'{source_dir} not found')
    if not dest_dir.exists():
        raise FileNotFoundError(f'{dest_dir} not found')

    # n_jobs = multiprocessing.cpu_count()
    n_jobs = 4
    files = [x for x in source_dir.iterdir() if x.is_file()]
    source_files = set([x.stem for x in files])
    dest_files = set([x.name for x in dest_dir.iterdir() if x.is_file()])
    unprocessed = [source_dir / f'{x}.apk' for x in source_files - dest_files]
    print(
        f"Only {len(unprocessed)} out of {len(source_files)} remain to be processed")
    print(f"Starting dataset processing with {n_jobs} Jobs")
    J.Parallel(n_jobs=n_jobs)(J.delayed(process)(x, dest_dir, i, len(unprocessed))
                              for i, x in enumerate(unprocessed))
    print("DONE")
