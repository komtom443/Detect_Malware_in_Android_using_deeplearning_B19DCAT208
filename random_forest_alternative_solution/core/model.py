from typing import Any, Mapping
from typing import Tuple, Optional, Dict

import dgl
import dgl.nn.pytorch as graph_nn
import pytorch_lightning as pl
import torchmetrics as metrics
import torch
import torch.nn.functional as F
from dgl.nn import Sequential
from torchmetrics import Metric
from torch import nn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import log_loss


class RandomDataDetector(pl.LightningModule):
    def __init__(self) -> None:
        super(RandomDataDetector, self).__init__()
        self.save_hyperparameters()
        self.model = RandomForestClassifier()
        self.loss_func = nn.BCEWithLogitsLoss()
        parameters = self.model.get_params()

        self.bootstrap = parameters['bootstrap']
        self.ccp_alpha = parameters['ccp_alpha']
        self.class_weight = parameters['class_weight']
        self.criterion = parameters['criterion']
        self.max_depth = parameters['max_depth']
        self.max_features = parameters['max_features']
        self.max_leaf_nodes = parameters['max_leaf_nodes']
        self.max_samples = parameters['max_samples']
        self.min_impurity_decrease = parameters['min_impurity_decrease']
        self.min_samples_leaf = parameters['min_samples_leaf']
        self.min_samples_split = parameters['min_samples_split']
        self.min_weight_fraction_leaf = parameters[
            'min_weight_fraction_leaf']
        self.n_estimators = parameters['n_estimators']
        self.n_jobs = parameters['n_jobs']
        self.oob_score = parameters['oob_score']
        self.random_state = parameters['random_state']
        self.verbose = parameters['verbose']
        self.warm_start = parameters['warm_start']

    @staticmethod
    def _get_metric_dict(stage: str) -> Mapping[str, Metric]:
        return nn.ModuleDict({
            f'{stage}_accuracy': metrics.Accuracy(task='binary'),
            f'{stage}_precision': metrics.Precision(num_classes=1, task='binary'),
            f'{stage}_recall': metrics.Recall(num_classes=1, task='binary'),
            f'{stage}_f1': metrics.FBetaScore(num_classes=1, task='binary')
        })

    def forward(self, x) -> torch.Tensor:
        return self.model.predict(x)

    def training_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int) -> torch.Tensor:
        bg, label = batch
        bg = bg.ndata['function_name']
        self.model.fit(bg, label)
        label_pred = self.model.predict(bg)

        loss = log_loss(label, label_pred)
        acc = accuracy_score(label, label_pred)

        self.log('train_loss', loss, on_step=True, on_epoch=True)
        self.log('train_accuracy', acc, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        bg = bg.ndata['function_name']
        self.model.fit(bg, label)
        label_pred = self.model.predict(bg)

        loss = log_loss(label, label_pred)
        acc = accuracy_score(label, label_pred)

        self.log('val_loss', loss, on_step=True, on_epoch=True)
        self.log('val_accuracy', acc, on_step=True, on_epoch=True)
        return loss

    def configure_optimizers(self) -> torch.optim.Adam:
        optimizer = torch.optim.SGD(self.parameters(), lr=0)
        return optimizer


print(pl.__version__)
print(torch.__version__)
